{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929680ed-a1d5-4e23-923f-cf093b9a3659",
   "metadata": {},
   "source": [
    "**Welcome \n",
    "- **The chatbot should look at your question and look around the internet for some resources.**\n",
    "- **Based on those resources, the chatbot should make an educated guess based on its retrieved information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3524c160-7261-445d-b624-90927f69c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://nim-llm:8000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581704cf-5fc4-43ce-93f8-ae5f9a1738e6",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 1:** Define The Planner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3da50-2dcd-4263-9b26-41a92fb2d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "from course_utils import SCHEMA_HINT\n",
    "\n",
    "\n",
    "## Create an LLM client with the sole intention of generating a plan.\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    steps: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f92897c2-e114-462c-98a4-667aabf3766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Instantiate a PydanticOutputParser for the Plan model\n",
    "plan_parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "\n",
    "# Get format instructions (schema hint) for the Plan output\n",
    "schema_hint = plan_parser.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6eb27d30-b781-4e1c-aa86-4b47b3b06500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escape curly braces in schema_hint to prevent formatting errors\n",
    "escaped_schema_hint = schema_hint.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "# Final planning prompt with improved prompt engineering\n",
    "planning_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"\"\"You are a master planner system who charts out a plan for how to solve a problem.\n",
    "\n",
    "Only respond with a valid JSON object matching the format below.\n",
    "Do NOT include any commentary, explanation, or additional text outside the JSON.\n",
    "\n",
    "{escaped_schema_hint}\n",
    "\"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "030ec68d-6ef1-4427-94df-0f96872f9bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Read LangGraph documentation to understand its purpose and functionality', 'Explore LangGraph website and download resources related to the project', 'Watch LangGraph tutorials and demonstrations on YouTube or other video sharing platforms', 'Familiarize yourself with the programming languages and technologies used in LangGraph ', 'Join online communities or forums to ask questions and interact with other LangGraph users', 'Experiment with LangGraph tools and libraries to gain hands-on experience', 'Read case studies or examples of projects that have successfully used LangGraph to solve real-world problems', 'Participate in LangGraph community events, meetups, or hackathons to network with other users and developers']\n"
     ]
    }
   ],
   "source": [
    "planning_chain = planning_prompt | llm | plan_parser\n",
    "\n",
    "# Step 6: Run the chain\n",
    "input_msgs = {\"input\": \"Can you help me learn more about LangGraph?\"}\n",
    "plan = planning_chain.invoke(input_msgs)\n",
    "\n",
    "# Step 7: Print output\n",
    "print(plan.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cb811-5c88-4454-9034-132eccf3ffb5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23e178-53cf-4965-9b8c-c52065a4be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- \n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "def generate_thoughts(input_msgs, config=None):\n",
    "    step_buffer = [\"\"]\n",
    "    for chunk in planning_chain.stream(input_msgs, config=config):\n",
    "        if \"steps\" in chunk and chunk.get(\"steps\"):\n",
    "            if len(chunk.get(\"steps\")) > len(step_buffer):\n",
    "                yield step_buffer[-1]\n",
    "                step_buffer += [\"\"]\n",
    "            dlen = len(chunk.get(\"steps\")[-1]) - len(step_buffer[-1])\n",
    "            step_buffer[-1] = chunk.get(\"steps\")[-1]\n",
    "    yield step_buffer[-1]\n",
    "    print(\"FINISHED\", flush=True)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "for thought in generate_thoughts(input_msgs):\n",
    "    \n",
    "    print(\"-\", thought)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fa5b0-1423-490a-9861-fbc2d85508d5",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Define The Retrieval Sub-Process Mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0f40e7bc-4976-47f6-9f23-0a0e10a5b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "import functools\n",
    "\n",
    "# Step 1: Optional caching of internet search\n",
    "@functools.cache\n",
    "def search_internet(query: str):\n",
    "    search = DuckDuckGoSearchAPIWrapper()\n",
    "    results = search.run(query)\n",
    "    return results\n",
    "\n",
    "# Step 2: Create a Runnable that wraps the search logic\n",
    "search_tool = RunnableLambda(lambda step: search_internet(step))\n",
    "\n",
    "# Step 3: Batch-style research process\n",
    "def research_options(steps):\n",
    "    valid_steps = [step.strip() for step in steps if isinstance(step, str) and step.strip()]\n",
    "    return search_tool.batch(valid_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "059aec92-076d-4679-b016-e429da3d5ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:176: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia import NVIDIARerank\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize Reranker\n",
    "reranker = NVIDIARerank()\n",
    "\n",
    "# Rerank retrieved search results using the original query (step)\n",
    "def retrieve_via_query(context_rets, query: str, k=5):\n",
    "    documents = [Document(page_content=ret) for ret in context_rets]\n",
    "    ranked_docs = reranker.compress_documents(documents=documents, query=query)\n",
    "    return [doc.page_content for doc in ranked_docs[:k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a3dea-dfe0-4a16-8482-d792067a0b1a",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 3:** Creating The Research Pipeline\n",
    "\n",
    "Now that we have some minimum-viable semblance of a supervisor/subordinate system, let's go ahead and orchestrate them in an interesting way. Feel free to come up with your own mechanism for \"reasoning\" about the question and \"researching\" the results. If you don't see a straightforward way to make it work, a default pool of prompts is offered below (possibly the ones we used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5751b561-393a-4083-978f-764fa165bdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "\n",
      "************************************************************\n",
      "✅ Final Answer:\n",
      "\n",
      "LangGraph is a graph-based language model that has been gaining attention in the natural language processing (NLP) community recently. Here's an overview:\n",
      "\n",
      "LangGraph is a hierarchical graph-based language model that was introduced in a 2021 research paper by a team of researchers from Google and the MIT-IBM Watson AI Lab ( Lewis et al., 2021). The model is designed to capture the hierarchical structure of language, which includes both local and long-range dependencies between words in a sentence.\n",
      "\n",
      "Key Features of LangGraph:\n",
      "\n",
      "1. **Hierarchical Structure**: LangGraph represents a sentence as a graph, where each node is a word and edges between nodes capture the relationships between words. This hierarchical structure allows the model to capture both local and long-range dependencies.\n",
      "2. **Graph-Based Encoder**: The LangGraph model uses a graph-based encoder to process the input sentence. This encoder is designed to effectively capture the complex relationships between words in the sentence.\n",
      "3. **Language Modeling and Generation**: LangGraph is trained on a language modeling task, where it predicts the next word in a sequence given the context of the previous words. The model can also be fine-tuned for generation tasks, such as text summarization and dialogue generation.\n",
      "\n",
      "Applications of LangGraph:\n",
      "\n",
      "1. **Text Summarization**: LangGraph has been shown to be effective for text summarization tasks, as it can capture the essential information in a document and generate a concise summary.\n",
      "2. **Dialogue Generation**: LangGraph can be used to generate human-like responses in dialogue systems, as it can capture the context and relationships between words in a conversation.\n",
      "3. **Question Answering**: LangGraph can be fine-tuned for question answering tasks, such as answering multiple-choice questions or generating answers to open-ended questions.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* Lewis, P., Liu, Y., Goyal, N., Chiu, C. L., Zohren, S., & Manning, C. (2021). LangGraph: A Hierarchical Graph-Based Language Model for NLP Tasks. arXiv preprint arXiv:2112.01659.\n",
      "\n",
      "Retrieved from: https://arxiv.org/abs/2112.01659\n",
      "\n",
      "Note: This is a concise overview of LangGraph, and it's recommended to read the original research paper for a more detailed understanding of the model.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"\"\"You are an agent tasked with providing an accurate, concise, and informative answer to a user’s query.\n",
    "        You will be given a question, a structured plan with steps, and retrievals containing relevant context from internet searches.\n",
    "        Provide a detailed yet concise answer that directly addresses the user's original question.\n",
    "        Explicitly cite the most relevant sources from the provided context at the end.\"\"\"\n",
    "    ),\n",
    "    (\"human\", \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Structured Plan:\n",
    "{steps}\n",
    "\n",
    "Retrieved Context:\n",
    "{retrievals}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# Define reasoning pipeline\n",
    "research_pipeline = agent_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Example usage\n",
    "question = \"Can you help me learn more about LangGraph?\"\n",
    "\n",
    "# Generate the structured plan\n",
    "input_msgs = {\"input\": question}\n",
    "sequence_of_actions = [thought for thought in generate_thoughts(input_msgs)]\n",
    "\n",
    "# Perform internet searches for each step\n",
    "search_retrievals = research_options(sequence_of_actions)\n",
    "\n",
    "# Rerank retrievals for each step\n",
    "filtered_results = [retrieve_via_query(search_retrievals, step) for step in sequence_of_actions]\n",
    "\n",
    "# Prepare final input\n",
    "final_input = {\n",
    "    \"question\": question,\n",
    "    \"steps\": \"\\n\".join(sequence_of_actions),\n",
    "    \"retrievals\": \"\\n\\n\".join(\"\\n\".join(res) for res in filtered_results)\n",
    "}\n",
    "\n",
    "# Invoke the research pipeline\n",
    "answer = research_pipeline.invoke(final_input)\n",
    "\n",
    "# Output final answer\n",
    "print(\"\\n\" + \"*\" * 60)\n",
    "print(\"✅ Final Answer:\\n\")\n",
    "print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cb376b57-8e58-4b4a-9676-c3a38a8d0909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "****************************************************************\n",
      "LangGraph is a graph-based language model that has been gaining attention in the natural language processing (NLP) community recently. Here's an overview\n",
      "LangGraph is a hierarchical graph-based language model that was introduced in a 2021 research paper by a team of researchers from Google and the MIT-IBM Watson AI Lab ( Lewis et al., 2021). The model is designed to capture the hierarchical structure of language, which includes both local and long-range dependencies between words in a sentence\n",
      "Key Features of LangGraph\n",
      "1. **Hierarchical Structure**: LangGraph represents a sentence as a graph, where each node is a word and edges between nodes capture the relationships between words. This hierarchical structure allows the model to capture both local and long-range dependencies\n",
      "2. **Graph-Based Encoder**: The LangGraph model uses a graph-based encoder to process the input sentence. This encoder is designed to effectively capture the complex relationships between words in the sentence\n",
      "3. **Language Modeling and Generation**: LangGraph is trained on a language modeling task, where it predicts the next word in a sequence given the context of the previous words. The model can also be fine-tuned for generation tasks, such as text summarization and dialogue generation\n",
      "Applications of LangGraph\n",
      "1. **Text Summarization**: LangGraph has been shown to be effective for text summarization tasks, as it can capture the essential information in a document and generate a concise summary\n",
      "2. **Dialogue Generation**: LangGraph can be used to generate human-like responses in dialogue systems, as it can capture the context and relationships between words in a conversation\n",
      "3. **Question Answering**: LangGraph can be fine-tuned for question answering tasks, such as answering multiple-choice questions or generating answers to open-ended questions\n",
      "Sources\n",
      "* Lewis, P., Liu, Y., Goyal, N., Chiu, C. L., Zohren, S., & Manning, C. (2021). LangGraph: A Hierarchical Graph-Based Language Model for NLP Tasks. arXiv preprint arXiv:2112.01659\n",
      "Retrieved from: https://arxiv.org/abs/2112.01659\n",
      "Note: This is a concise overview of LangGraph, and it's recommended to read the original research paper for a more detailed understanding of the model."
     ]
    }
   ],
   "source": [
    "question = \"Can you help me learn more about LangGraph?\"\n",
    "input_msgs = {\"messages\": [(\"user\", question)]}\n",
    "\n",
    "# Generate structured steps\n",
    "sequence_of_actions = [thought for thought in generate_thoughts({\"input\": question})]\n",
    "\n",
    "# Perform retrievals and reranking for each step\n",
    "search_retrievals = research_options(sequence_of_actions)\n",
    "filtered_results = [retrieve_via_query(search_retrievals, step) for step in sequence_of_actions]\n",
    "\n",
    "# Accumulate intermediate question-answer pairs progressively\n",
    "progressive_reasoning = []\n",
    "for action, result in zip(sequence_of_actions, filtered_results):\n",
    "    progressive_reasoning.append(f\"Step: {action}\\nResult: {' '.join(result)}\\n\")\n",
    "\n",
    "# Concatenate the reasoning into final input\n",
    "input_msgs[\"messages\"].append((\"assistant\", \"\\n\".join(progressive_reasoning)))\n",
    "\n",
    "# Now input_msgs[\"messages\"] contains accumulated reasoning steps\n",
    "print(\"*\" * 64)\n",
    "final_input = {\n",
    "    \"question\": question,\n",
    "    \"steps\": \"\\n\".join(sequence_of_actions),\n",
    "    \"retrievals\": \"\\n\\n\".join(\"\\n\".join(res) for res in filtered_results)\n",
    "}\n",
    "\n",
    "for token in research_pipeline.stream(final_input):\n",
    "    if \"\\n\" in token:\n",
    "        print(flush=True)\n",
    "    else:\n",
    "        print(token, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2063c7-b8b0-4cc0-9e63-25dab62f6c43",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 4:** Accumulating Your Reasoning Traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "270640e7-702c-4a60-81f3-665359ecf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Aggregate 8 question-trace-answer triples. \n",
    "submission = [\n",
    "    {\n",
    "        \"question\": \"Can you help me learn more about LangGraph?\",\n",
    "        \"trace\": sequence_of_actions,\n",
    "        \"answer\": answer\n",
    "    },\n",
    "    # Repeat the process with other specialized questions below\n",
    "    {\n",
    "        \"question\": \"Can you help me learn more about LangGraph? Specifically, can you tell me about Memory Management?\",\n",
    "        \"trace\": \"...\",  # replace with actual reasoning trace\n",
    "        \"answer\": \"...\"  # replace with actual final answer\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you help me learn more about LangGraph? Specifically, can you tell me about Pregel?\",\n",
    "        \"trace\": \"...\",  \n",
    "        \"answer\": \"...\" \n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you help me learn more about LangGraph? Specifically, can you tell me about subgraphs?\",\n",
    "        \"trace\": \"...\",\n",
    "        \"answer\": \"...\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you help me learn more about LangGraph? Specifically, can you tell me about full-duplex communication?\",\n",
    "        \"trace\": \"...\",\n",
    "        \"answer\": \"...\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you help me learn more about LangGraph? Specifically, can you tell me about productionalization?\",\n",
    "        \"trace\": \"...\",\n",
    "        \"answer\": \"...\"\n",
    "    },\n",
    "    # Add two of your own specialized questions here:\n",
    "    {\n",
    "        \"question\": \"Your specialized question 1?\",\n",
    "        \"trace\": \"...\",\n",
    "        \"answer\": \"...\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Your specialized question 2?\",\n",
    "        \"trace\": \"...\",\n",
    "        \"answer\": \"...\"\n",
    "    }\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
